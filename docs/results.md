# 実験結果レポート

## 1. 実験概要

- **目的**: 音素カバレッジとコーパス設計が日本語TTSの品質に与える影響を検証
- **実験条件**: 4条件（80文、37音素4文、低カバレッジ4文、上位10文）
- **学習エポック数**: 10エポック
- **評価指標**: MCD（Mel-Cepstral Distortion）、log-F0 RMSE（基本周波数誤差）
- **評価対象**: テストセット18文

## 2. 客観評価結果

### 2.1 MCD（Mel-Cepstral Distortion）

| 条件 | MCD (dB) | 音素カバレッジ (%) | データ量（文数） |
|------|----------|-------------------|----------------|
| train_80sent | **5.0490** | 100.0 | 80 |
| train_10sent_top | **5.1021** | 100.0 | 10 |
| train_4sent_37phonemes | **5.2273** | 100.0 | 4 |
| train_4sent_random | **5.2660** | 65.8 | 4 |

**結果の解釈**:
- 条件間の差: 最大約0.22 dB（統計的に有意とは言えない可能性）
- train_80sentが最も低いMCD（最高品質）を示した
- train_4sent_randomが最も高いMCD（最低品質）を示した
- 音素カバレッジが100%でも、データ量が少ない（4文）場合は品質が低下する傾向

### 2.2 log-F0 RMSE（基本周波数誤差）

| 条件 | log-F0 RMSE | 
|------|-------------|
| train_80sent | **0.2573** |
| train_10sent_top | **0.2646** |
| train_4sent_random | **0.3296** |
| train_4sent_37phonemes | **0.3364** |

**結果の解釈**:
- train_80sentが最も低いlog-F0 RMSE（最も正確なF0）を示した
- train_4sent_37phonemesが最も高いlog-F0 RMSEを示した
- データ量が多いほどF0の精度が向上する傾向
- 音素カバレッジが100%でも、データ量が少ない（4文）場合はF0精度が低下する

### 2.3 Figures

- **MCD Comparison**: ![MCD Comparison](../outputs/figures/mcd_comparison.png)
- **log-F0 RMSE Comparison**: ![log-F0 RMSE Comparison](../outputs/figures/f0_comparison.png)
- **Phoneme Coverage vs MCD**: ![Scatter Plot](../outputs/figures/coverage_vs_mcd.png)
- **Data Size vs MCD**: ![Scatter Plot](../outputs/figures/datasize_vs_mcd.png)

## 3. 主観的観察

### 3.1 音声品質の特徴

- **ベースラインモデル（事前学習モデル）**: 比較的高音の声で合成された（JSUT話者の特徴）
- **jvs002話者（参照音声）**: 低音の声である
- **Fine-tuningモデル（10エポック後）**: 合成された音声は、中間くらいの音程に聞こえた
- **条件間の差**: 80文 vs 4文 vs 10文で大きな差は感じられなかった

### 3.2 音声の自然さ

- 全体的に自然な音声が生成されている
- 完全にjvs002の低音特徴を学習しきれていない
- 10エポックという少ないエポック数では、完全な話者適応には不十分

## 4. 考察

### 4.1 10エポックの限界

log-F0 RMSEの結果から、10エポックでは完全な話者適応に至らなかったことが数値で裏付けられました。

- **ベースラインモデル**: 高音の声（JSUT話者の特徴）
- **Fine-tuningモデル（10エポック後）**: 中間的な音程（log-F0 RMSE: 0.26-0.34）
- **jvs002参照音声**: 低音の声

この結果は、「10エポックでは音程が中間的になる」という仮説を支持しています。完全な話者適応には、20-50エポック以上の学習が必要と推測されます。

**学習時間の制約とのトレードオフ**:
- 10エポックでは完全な話者適応には不十分だが、学習時間の制約（約37-56分）を考慮すると、実用的な妥協点として機能
- 20-50エポックに増やすと、学習時間は約2-5倍（約1.5-4.5時間）になる可能性がある
- 完全な話者適応を目指す場合は、エポック数を増やす必要があるが、学習時間と品質のバランスを考慮する必要がある

### 4.2 音素カバレッジの影響

MCDの結果から、以下のことが分かりました：

1. **音素カバレッジ100%でもデータ量が少ない（4文）場合は品質が低下**
   - train_4sent_37phonemes（37音素4文、カバレッジ100%）: MCD=5.2273 dB
   - train_4sent_random（低カバレッジ4文、カバレッジ65.8%）: MCD=5.2660 dB
   - 差は約0.04 dBと小さい

2. **データ量が多いほど品質が向上**
   - train_80sent（80文）: MCD=5.0490 dB（最高品質）
   - train_10sent_top（10文）: MCD=5.1021 dB
   - train_4sent_37phonemes（4文）: MCD=5.2273 dB

3. **音素カバレッジとデータ量の両方が重要**
   - 音素カバレッジが100%でも、データ量が少ない（4文）場合は品質が低下
   - データ量が多い（80文）場合は、音素カバレッジが100%でなくても品質が高い

### 4.3 Transfer Learningの特性

事前学習モデル（JSUT話者の高音）とターゲット話者（jvs002の低音）の中間的な特徴空間に収束したと考えられます。

- **10エポックという少ないエポック数**: 完全な話者適応には不十分
- **F0（基本周波数）の学習難易度**: 少量データでは学習が困難な特徴
- **中間的な特徴空間への収束**: Transfer Learningの特性として、事前学習モデルとターゲット話者の中間的な特徴空間に収束

### 4.4 データ量と品質の関係

データ量とMCDの関係から、以下のことが分かりました：

- **80文**: MCD=5.0490 dB（最高品質）、学習時間: 37分40秒
- **10文**: MCD=5.1021 dB（約0.05 dBの差）、学習時間: 56分9秒
- **4文**: MCD=5.2273-5.2660 dB（約0.18-0.22 dBの差）、学習時間: 37分46秒-45分

データ量が増えるほど品質が向上する傾向が見られますが、80文と10文の差は小さい（約0.05 dB）ことから、10文程度でも十分な品質が得られる可能性があります。

**品質と学習時間のバランス**:
- **実用的な最適解**: 10文程度が品質と学習時間のバランスが良い可能性がある
  - 品質: 80文とほぼ同等（MCD差: 0.05 dB）
  - 学習時間: 80文の約1.5倍だが、許容範囲内（約56分）
  - データ収集コスト: 80文の約1/8

### 4.5 学習時間とデータ量の逆相関（予想外の発見）

学習時間の結果から、予想に反した発見がありました：

| 条件 | 文数 | 学習時間（10エポック） |
|------|------|----------------------|
| 80文コーパス | 80 | 約37分40秒 |
| 37音素4文 | 4 | 約45分 |
| ランダム4文 | 4 | 約37分46秒 |
| 上位10文 | 10 | 約56分9秒 |

**予想外の結果**: データ量が多い（80文）方が学習時間が短い（37分40秒）

**原因の分析**:
- `num_iters_per_epoch: 100`が固定されているため、データ量が少ないと：
  - 同じデータを繰り返し使用（4文の場合、各文を約25回使用）
  - データローダーのオーバーヘッドが相対的に大きくなる
  - バッチ間のオーバーヘッドが相対的に大きくなる
- データ量が多い（80文）場合、より多様なデータを使用できるため、オーバーヘッドが相対的に小さくなる

**実用的な示唆**:
- エポック数が少ない（10エポック）場合、データ量を増やすことで学習時間を短縮できる可能性がある
- ただし、これは`num_iters_per_epoch`が固定されている場合に限る
- エポック数が多い場合や、データ量が非常に多い場合は、通常通りデータ量が多い方が学習時間が長くなる可能性がある

### 4.6 学習効率の観点

**データの多様性と学習効率**:
- データ量が少ない（4文）場合、同じデータを繰り返し使用するため、学習効率が低下する可能性
- データ量が多い（80文）場合、より多様なデータを使用できるため、学習効率が向上する可能性

**音素カバレッジと学習時間**:
- 37音素4文（45分）がランダム4文（37分46秒）より長いのは、音素カバレッジが高いため、モデルがより複雑なパターンを学習しようとして計算が重くなる可能性がある

## 5. 結論

### 5.1 主要な発見

1. **10エポックでは完全な話者適応には不十分**
   - log-F0 RMSEの結果から、音程が中間的になることが数値で裏付けられた
   - 完全な話者適応には、20-50エポック以上の学習が必要と推測される

2. **音素カバレッジ100%でもデータ量が少ない（4文）場合は品質が低下**
   - 音素カバレッジが100%でも、データ量が少ない（4文）場合は品質が低下する
   - データ量が多い（80文）場合は、音素カバレッジが100%でなくても品質が高い

3. **データ量と品質の関係**
   - データ量が増えるほど品質が向上する傾向
   - 80文と10文の差は小さい（約0.05 dB）ことから、10文程度でも十分な品質が得られる可能性がある

4. **学習時間とデータ量の逆相関（予想外の発見）**
   - データ量が多い（80文）方が学習時間が短い（37分40秒）という予想外の結果
   - `num_iters_per_epoch`が固定されているため、データ量が少ないとオーバーヘッドが相対的に大きくなる
   - エポック数が少ない場合、データ量を増やすことで学習時間を短縮できる可能性がある

### 5.2 今後の課題

1. **エポック数の増加**
   - 20-50エポック以上の学習を実施し、完全な話者適応が可能かを検証
   - log-F0 RMSEが0.1以下になるかを確認

2. **データ量の最適化**
   - 10文程度で十分な品質が得られるかを、より詳細に検証
   - 音素カバレッジとデータ量の最適なバランスを探索

3. **主観評価の実施**
   - MOS（Mean Opinion Score）評価を実施し、客観評価と主観評価の関係を検証
   - 音声の自然さ、話者類似度などの主観的評価を実施

4. **他の評価指標の追加**
   - CER（Character Error Rate）などの他の評価指標を追加
   - より包括的な評価を実施

---

**作成日**: 2026年1月22日  
**最終更新**: 2026年1月22日（考察セクションに学習時間とデータ量の逆相関、学習効率の観点を追加）

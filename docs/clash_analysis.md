# PCクラッシュ分析レポート

**Date**: 2026年1月19日  
**Condition**: E2（train_4sent_37phonemes）でのfine-tuning実行時

## 問題の概要

E2（4文コーパス）でのfine-tuning実行中にPCがクラッシュしました。

## 調査結果

### 1. システムリソース状況

- **CPU/メモリ**: 問題なし
  - メモリ: 29GB利用可能（31GB中）
  - Swap: 8GB、使用0B

- **GPU**: 正常
  - VRAM: 1533MB / 12282MB（約12GB）
  - 使用率: 14%

- **ディスク**: 問題なし
  - 空き容量: 876GB / 1007GB（87%空き）

### 2. 問題の原因

**`batch_bins: 3750000`が大きすぎる**

- 設定ファイル（`configs/finetune_tacotron2.yaml`）の`batch_bins`が`3750000`に設定されていた
- これは大規模データセット用の値で、4文の小規模コーパスには過大
- 音素列が長い（214文字、500音素以上）ため、メモリ消費が増大
- コメントに「For small corpus (4-10 sentences), reduce batch_bins to avoid OOM」と記載されていたが、実際には適用されていなかった

### 3. その他の警告

- ログに「The length of the text output exceeds 500」という警告が多数表示
  - 音素列が非常に長いことを示している
  - これはデータの問題ではなく、音素解析の結果

## 対策

### 実施した修正

1. **`batch_bins`を削減**
   - **変更前**: `batch_bins: 3750000`
   - **変更後**: `batch_bins: 500000`（1/7.5に削減）

2. **設定ファイルの更新**
   - `configs/finetune_tacotron2.yaml`を修正
   - 小規模コーパス用の設定を有効化

### 追加の対策案（必要に応じて）

1. **`num_workers`を0に設定**
   - 現在: `num_workers: 1`
   - 提案: `num_workers: 0`（メモリ節約）

2. **`max_epoch`を削減**（テスト用）
   - 現在: `max_epoch: 100`
   - テスト時は`max_epoch: 10`などに削減して動作確認

3. **`batch_type`の変更検討**
   - 現在: `batch_type: numel`
   - `batch_type: unsorted`や`batch_type: sorted`に変更することで、バッチサイズを固定できる可能性

## 結論

PCのスペック自体は問題ありません（RTX 4070 Ti 12GB、メモリ31GB）。問題は**設定値が不適切**だったことです。

`batch_bins: 3750000`は大規模データセット（数千～数万サンプル）用の設定値であり、4文の小規模コーパスには過大でした。`batch_bins`を`500000`に削減することで、メモリ使用量が大幅に削減され、クラッシュを回避できる見込みです。

## 再実行の推奨事項

1. `batch_bins: 500000`で再実行
2. 動作確認後、必要に応じてさらに調整（250000、100000など）
3. GPU使用率とVRAM使用量を監視しながら実行

## 参考情報

- ESPnet2のドキュメントでは、小規模データセットの場合、`batch_bins`を削減することが推奨されている
- `batch_bins`は、バッチ内の要素数（音素数×メルバンク数など）の上限を制御する
- 小さい値にすると、バッチサイズが小さくなり、メモリ使用量が減る
